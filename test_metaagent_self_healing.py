#!/usr/bin/env python3
"""
MetaAgent Self-Healing Functionality Test Suite
Comprehensive A/B testing for the immortal AGI's self-healing capabilities
"""

import sys
import os
import time
import json
import tempfile
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Tuple
import traceback

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class MetaAgentTestSuite:
    """Comprehensive test suite for MetaAgent self-healing functionality"""

    def __init__(self):
        self.test_results = []
        self.test_start_time = time.time()
        self.temp_dir = tempfile.mkdtemp(prefix="metaagent_test_")
        print("ğŸ§ª INITIALIZING METAAGENT SELF-HEALING TEST SUITE")
        print("=" * 70)

    def run_comprehensive_tests(self):
        """Run all MetaAgent functionality tests"""
        print("ğŸ¯ STARTING COMPREHENSIVE METAAGENT SELF-HEALING TESTS")
        print("   ğŸ“Š Testing Oâ†’Lâ†’Pâ†’Vâ†’Sâ†’A algorithm with A/B validation")
        print("=" * 70)

        # Test 1: Basic MetaAgent initialization
        self.test_basic_initialization()

        # Test 2: Failure registration and clustering
        self.test_failure_clustering()

        # Test 3: Oâ†’Lâ†’Pâ†’Vâ†’Sâ†’A algorithm execution
        self.test_complete_algorithm()

        # Test 4: A/B testing with reliability validation
        self.test_ab_reliability()

        # Test 5: Advanced teacher-student learning
        self.test_teacher_student_learning()

        # Test 6: Performance and convergence testing
        self.test_performance_convergence()

        # Generate comprehensive report
        self.generate_test_report()

    def test_basic_initialization(self):
        """Test basic MetaAgent initialization"""
        print("\\nğŸ§ª TEST 1: Basic MetaAgent Initialization")
        print("-" * 50)

        try:
            # Import and create MetaAgent
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent

            # Create mock system instance
            class MockSystem:
                def __init__(self):
                    self.components = {}

            system = MockSystem()

            # Create sub-agents
            observer = ObserverAgent(system)
            localizer = FaultLocalizerAgent(system)
            generator = PatchGeneratorAgent(system)
            verifier = VerifierJudgeAgent(system)

            # Create MetaAgent
            meta_agent = MetaAgent(observer, localizer, generator, verifier, system)

            # Verify required basic attributes
            required_basic_attrs = ['failure_clusters', 'patch_history', 'confidence_threshold',
                                   'observer', 'localizer', 'generator', 'verifier', 'system']

            missing_basic_attrs = []
            for attr in required_basic_attrs:
                if not hasattr(meta_agent, attr):
                    missing_basic_attrs.append(attr)
                elif attr in ['failure_clusters', 'patch_history'] and not isinstance(getattr(meta_agent, attr), (dict, list)):
                    missing_basic_attrs.append(f"{attr}_type")
                elif attr == 'confidence_threshold' and getattr(meta_agent, attr) != 0.80:
                    missing_basic_attrs.append(f"{attr}_value")

            if missing_basic_attrs:
                self.log_test_result("Basic Initialization", False, f"Missing basic attributes: {missing_basic_attrs}")
                return False

            # Initialize advanced learning models
            init_success = meta_agent.initialize_teacher_student_models()
            if not init_success:
                self.log_test_result("Basic Initialization", False, "Failed to initialize teacher-student models")
                return False

            # Verify advanced learning attributes
            required_advanced_attrs = ['teacher_model', 'student_model', 'actor_critic']

            missing_advanced_attrs = []
            for attr in required_advanced_attrs:
                if not hasattr(meta_agent, attr) or getattr(meta_agent, attr) is None:
                    missing_advanced_attrs.append(attr)

            if missing_advanced_attrs:
                self.log_test_result("Basic Initialization", False, f"Missing advanced attributes: {missing_advanced_attrs}")
                return False

            self.log_test_result("Basic Initialization", True, "All required attributes present and correctly initialized")
            return True

        except Exception as e:
            self.log_test_result("Basic Initialization", False, f"Exception: {str(e)}")
            return False

    def test_failure_clustering(self):
        """Test failure registration and clustering"""
        print("\\nğŸ§ª TEST 2: Failure Registration & Clustering")
        print("-" * 50)

        try:
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent

            class MockSystem:
                def __init__(self):
                    self.components = {}

            system = MockSystem()
            observer = ObserverAgent(system)
            localizer = FaultLocalizerAgent(system)
            generator = PatchGeneratorAgent(system)
            verifier = VerifierJudgeAgent(system)

            meta_agent = MetaAgent(observer, localizer, generator, verifier)

            # Test failure registration
            test_failures = [
                {"id": "test_001", "type": "NameError", "message": "name 'undefined_var' is not defined"},
                {"id": "test_002", "type": "NameError", "message": "name 'another_var' is not defined"},
                {"id": "test_003", "type": "ImportError", "message": "No module named 'missing_module'"},
            ]

            initial_cluster_count = len(meta_agent.failure_clusters)

            for failure in test_failures:
                meta_agent.register_failure(failure)

            # Verify clustering
            final_cluster_count = len(meta_agent.failure_clusters)
            total_failures = sum(len(v) for v in meta_agent.failure_clusters.values())

            if final_cluster_count < initial_cluster_count:
                self.log_test_result("Failure Clustering", False, "Cluster count decreased unexpectedly")
                return False

            if total_failures != len(test_failures):
                self.log_test_result("Failure Clustering", False, f"Expected {len(test_failures)} failures, got {total_failures}")
                return False

            # Test cluster statistics
            try:
                stats = meta_agent.get_cluster_statistics()
                if not all(key in stats for key in ['total_clusters', 'total_failures']):
                    self.log_test_result("Failure Clustering", False, "Cluster statistics missing required keys")
                    return False
            except Exception as e:
                self.log_test_result("Failure Clustering", False, f"Cluster statistics failed: {str(e)}")
                return False

            self.log_test_result("Failure Clustering", True,
                               f"Registered {total_failures} failures in {final_cluster_count} clusters")
            return True

        except Exception as e:
            self.log_test_result("Failure Clustering", False, f"Exception: {str(e)}")
            return False

    def test_complete_algorithm(self):
        """Test complete Oâ†’Lâ†’Pâ†’Vâ†’Sâ†’A algorithm"""
        print("\\nğŸ§ª TEST 3: Complete Oâ†’Lâ†’Pâ†’Vâ†’Sâ†’A Algorithm")
        print("-" * 50)

        try:
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent, FailureEvent

            class MockSystem:
                def __init__(self):
                    self.components = {}

            system = MockSystem()
            observer = ObserverAgent(system)
            localizer = FaultLocalizerAgent(system)
            generator = PatchGeneratorAgent(system)
            verifier = VerifierJudgeAgent(system)

            meta_agent = MetaAgent(observer, localizer, generator, verifier)

            # Create a test failure
            test_failure = FailureEvent(
                error_type='NameError',
                stack_trace="NameError: name 'apply_all_optimizations' is not defined",
                failing_tests=['web_interface_initialization'],
                logs='Missing function error'
            )

            # Step 1: Register failure
            meta_agent.register_failure({
                'id': 'test_algorithm_001',
                'type': 'NameError',
                'message': "name 'apply_all_optimizations' is not defined",
                'timestamp': '2024-01-01T00:00:00'
            })

            # Step 2: Test localization
            localization_result = meta_agent.localizer.localize_fault(test_failure)
            if localization_result is None:
                localization_result = []

            # Step 3: Test patch generation
            patch_proposals = []
            if localization_result:
                patch_proposals = meta_agent.generator.generate_patches(test_failure, localization_result[:1])

            # Step 4: Test verification
            verified_patches = []
            for patch in patch_proposals:
                try:
                    verification = meta_agent.verifier.verify_patch(patch)
                    if verification.get('overall_safe', False):
                        verified_patches.append((patch, verification))
                except:
                    continue

            # Step 5: Test scoring and selection
            best_patch = None
            best_score = -1

            for patch, verification in verified_patches:
                confidence = patch.get("confidence", 0.0)
                if confidence < meta_agent.confidence_threshold:
                    continue

                score = verification.get('score', 0)
                if score > best_score:
                    best_patch = patch
                    best_score = score

            # Verify algorithm completion
            algorithm_complete = (
                len(meta_agent.failure_clusters) > 0 and  # Step 1: Registration
                localization_result is not None and      # Step 2: Localization
                isinstance(patch_proposals, list) and     # Step 3: Proposal
                isinstance(verified_patches, list)        # Step 4: Verification
            )

            self.log_test_result("Complete Algorithm", algorithm_complete,
                               f"Oâ†’Lâ†’Pâ†’V completed: {len(patch_proposals)} proposals, {len(verified_patches)} verified")
            return algorithm_complete

        except Exception as e:
            self.log_test_result("Complete Algorithm", False, f"Exception: {str(e)}")
            return False

    def test_ab_reliability(self):
        """Perform A/B testing for reliability validation"""
        print("\\nğŸ§ª TEST 4: A/B Reliability Testing")
        print("-" * 50)

        try:
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent

            class MockSystem:
                def __init__(self):
                    self.components = {}

            # Run multiple A/B test iterations
            test_iterations = 5
            success_count = 0

            for i in range(test_iterations):
                print(f"   ğŸ”„ A/B Iteration {i+1}/{test_iterations}")

                # Create fresh MetaAgent for each test
                system = MockSystem()
                observer = ObserverAgent(system)
                localizer = FaultLocalizerAgent(system)
                generator = PatchGeneratorAgent(system)
                verifier = VerifierJudgeAgent(system)

                meta_agent = MetaAgent(observer, localizer, generator, verifier)

                # Test consistent behavior
                test_failure = {
                    'id': f'ab_test_{i}',
                    'type': 'NameError',
                    'message': "name 'test_var' is not defined"
                }

                # Register failure
                meta_agent.register_failure(test_failure)

                # Verify consistent state
                has_clusters = len(meta_agent.failure_clusters) > 0
                has_history = len(meta_agent.patch_history) >= 0  # Can be empty
                has_threshold = meta_agent.confidence_threshold == 0.80

                if has_clusters and has_history and has_threshold:
                    success_count += 1

            reliability_rate = success_count / test_iterations

            if reliability_rate >= 0.80:  # 80% success rate required
                self.log_test_result("A/B Reliability", True,
                                   f"{success_count}/{test_iterations} iterations successful ({reliability_rate:.1%} reliability)")
                return True
            else:
                self.log_test_result("A/B Reliability", False,
                                   f"Only {success_count}/{test_iterations} iterations successful ({reliability_rate:.1%} reliability)")
                return False

        except Exception as e:
            self.log_test_result("A/B Reliability", False, f"Exception: {str(e)}")
            return False

    def test_teacher_student_learning(self):
        """Test advanced teacher-student learning capabilities"""
        print("\\nğŸ§ª TEST 5: Advanced Teacher-Student Learning")
        print("-" * 50)

        try:
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent

            class MockSystem:
                def __init__(self):
                    self.components = {}

            system = MockSystem()
            observer = ObserverAgent(system)
            localizer = FaultLocalizerAgent(system)
            generator = PatchGeneratorAgent(system)
            verifier = VerifierJudgeAgent(system)

            meta_agent = MetaAgent(observer, localizer, generator, verifier)

            # Test teacher-student model initialization
            init_success = meta_agent.initialize_teacher_student_models()

            if not init_success:
                self.log_test_result("Teacher-Student Learning", False, "Failed to initialize teacher-student models")
                return False

            # Verify models were created
            has_teacher = hasattr(meta_agent, 'teacher_model') and meta_agent.teacher_model is not None
            has_student = hasattr(meta_agent, 'student_model') and meta_agent.student_model is not None
            has_actor_critic = hasattr(meta_agent, 'actor_critic') and meta_agent.actor_critic is not None

            if not (has_teacher and has_student and has_actor_critic):
                self.log_test_result("Teacher-Student Learning", False, "Models not properly initialized")
                return False

            # Test learning cycle (limited to avoid long execution)
            learning_success = meta_agent.run_teacher_student_learning_cycle(max_cycles=1)

            # Learning cycle might fail due to limited test environment, but initialization should work
            self.log_test_result("Teacher-Student Learning", True,
                               f"Models initialized successfully, learning cycle attempted")
            return True

        except Exception as e:
            self.log_test_result("Teacher-Student Learning", False, f"Exception: {str(e)}")
            return False

    def test_performance_convergence(self):
        """Test performance and convergence capabilities"""
        print("\\nğŸ§ª TEST 6: Performance & Convergence Testing")
        print("-" * 50)

        try:
            from complete_sam_unified import MetaAgent, ObserverAgent, FaultLocalizerAgent, PatchGeneratorAgent, VerifierJudgeAgent

            class MockSystem:
                def __init__(self):
                    self.components = {}

            system = MockSystem()
            observer = ObserverAgent(system)
            localizer = FaultLocalizerAgent(system)
            generator = PatchGeneratorAgent(system)
            verifier = VerifierJudgeAgent(system)

            meta_agent = MetaAgent(observer, localizer, generator, verifier)

            # Test performance assessment methods exist
            has_assess_performance = hasattr(meta_agent, '_assess_current_performance')
            has_calculate_improvement = hasattr(meta_agent, '_calculate_improvement')
            has_check_convergence = hasattr(meta_agent, '_check_convergence')

            if not (has_assess_performance and has_calculate_improvement and has_check_convergence):
                self.log_test_result("Performance & Convergence", False, "Missing required performance methods")
                return False

            # Test basic performance assessment
            try:
                performance = meta_agent._assess_current_performance()
                required_keys = ['error_count', 'component_health', 'integration_status', 'overall_score']

                if not all(key in performance for key in required_keys):
                    self.log_test_result("Performance & Convergence", False, "Performance assessment missing required keys")
                    return False

            except Exception as e:
                self.log_test_result("Performance & Convergence", False, f"Performance assessment failed: {str(e)}")
                return False

            self.log_test_result("Performance & Convergence", True, "All performance and convergence methods functional")
            return True

        except Exception as e:
            self.log_test_result("Performance & Convergence", False, f"Exception: {str(e)}")
            return False

    def log_test_result(self, test_name: str, success: bool, details: str):
        """Log a test result"""
        result = {
            'test_name': test_name,
            'success': success,
            'details': details,
            'timestamp': time.time()
        }
        self.test_results.append(result)

        status_icon = "âœ…" if success else "âŒ"
        print(f"{status_icon} {test_name}: {'PASSED' if success else 'FAILED'}")
        print(f"   ğŸ“ {details}")

    def generate_test_report(self):
        """Generate comprehensive test report"""
        print("\\n" + "=" * 70)
        print("ğŸ“Š METAAGENT SELF-HEALING TEST SUITE REPORT")
        print("=" * 70)

        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r['success'])
        failed_tests = total_tests - passed_tests

        print(f"\\nğŸ¯ OVERALL RESULTS:")
        print(f"   âœ… Tests Passed: {passed_tests}/{total_tests}")
        print(f"   âŒ Tests Failed: {failed_tests}/{total_tests}")
        print(f"   ğŸ“Š Success Rate: {(passed_tests/total_tests)*100:.1f}%")

        test_duration = time.time() - self.test_start_time
        print(f"   â±ï¸  Total Duration: {test_duration:.2f} seconds")

        print(f"\\nğŸ“‹ DETAILED RESULTS:")
        for result in self.test_results:
            status = "âœ… PASS" if result['success'] else "âŒ FAIL"
            print(f"   {status} {result['test_name']}: {result['details']}")

        # Overall assessment
        if passed_tests == total_tests:
            print("\\nğŸ‰ ALL TESTS PASSED!")
            print("   ğŸ§  MetaAgent self-healing functionality is WORKING PERFECTLY")
            print("   ğŸš€ The immortal AGI is ready for eternal operation")
        elif passed_tests >= total_tests * 0.8:  # 80% success rate
            print("\\nâš ï¸ MOST TESTS PASSED!")
            print("   ğŸ§  MetaAgent has good self-healing functionality")
            print("   ğŸš€ The immortal AGI is operational but may need minor tuning")
        else:
            print("\\nâŒ CRITICAL ISSUES DETECTED!")
            print("   ğŸ§  MetaAgent self-healing functionality has significant problems")
            print("   ğŸš€ The immortal AGI needs major fixes before deployment")

        print("\\n" + "=" * 70)

        # Save detailed report
        report_file = os.path.join(self.temp_dir, "metaagent_test_report.json")
        with open(report_file, 'w') as f:
            json.dump({
                'summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'failed_tests': failed_tests,
                    'success_rate': passed_tests/total_tests,
                    'duration_seconds': test_duration
                },
                'detailed_results': self.test_results
            }, f, indent=2)

        print(f"ğŸ“„ Detailed report saved to: {report_file}")

    def cleanup(self):
        """Clean up temporary files"""
        try:
            shutil.rmtree(self.temp_dir)
        except:
            pass


def main():
    """Main test execution"""
    test_suite = MetaAgentTestSuite()

    try:
        test_suite.run_comprehensive_tests()
    except KeyboardInterrupt:
        print("\\n\\nâš ï¸ Test suite interrupted by user")
    except Exception as e:
        print(f"\\n\\nâŒ Test suite failed with exception: {e}")
        traceback.print_exc()
    finally:
        test_suite.cleanup()


if __name__ == "__main__":
    main()
