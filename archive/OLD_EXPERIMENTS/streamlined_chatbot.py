#!/usr/bin/env python3
"""
Streamlined Chatbot System
Direct integration with existing LLM directory and SAM models
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path

class StreamlinedChatbot:
    def __init__(self):
        """Initialize the streamlined chatbot"""
        print("ğŸ¤– STREAMLINED CHATBOT SYSTEM")
        print("=" * 50)
        print("ğŸš€ Direct LLM + SAM integration")
        print("ğŸ’¬ No pretraining required")
        print("ğŸ§  Ready to chat immediately")
        
        # System paths
        self.base_path = Path("/Users/samueldasari/Personal/NN_C")
        self.llm_path = self.base_path / "ORGANIZED" / "LLM"
        self.sam_path = self.base_path / "ORGANIZED" / "UTILS" / "sam_agi"
        
        # Check system components
        self.check_system_status()
        
        # Initialize conversation
        self.conversation_history = []
        self.session_start = time.time()
        
    def check_system_status(self):
        """Check what components are available"""
        print(f"\nğŸ” Checking System Status...")
        
        # Check LLM directory
        self.llm_available = self.llm_path.exists()
        print(f"  ğŸ“š LLM Directory: {'âœ… Available' if self.llm_available else 'âŒ Not Found'}")
        
        # Check SAM model
        self.sam_available = self.sam_path.exists()
        print(f"  ğŸ§  SAM Model: {'âœ… Available' if self.sam_available else 'âŒ Not Found'}")
        
        # Check Ollama
        self.ollama_available = self.check_ollama()
        print(f"  ğŸ¤– Ollama: {'âœ… Available' if self.ollama_available else 'âŒ Not Available'}")
        
        # Check knowledge base
        kb_path = self.base_path / "KNOWLEDGE_BASE"
        self.knowledge_available = kb_path.exists()
        print(f"  ğŸ“– Knowledge Base: {'âœ… Available' if self.knowledge_available else 'âŒ Not Found'}")
        
    def check_ollama(self):
        """Check if Ollama is available"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    def query_ollama_direct(self, prompt, model="llama2", timeout=15):
        """Direct Ollama query without pretraining"""
        try:
            # Use shorter, more focused prompts for faster responses
            if len(prompt) > 200:
                prompt = prompt[:200] + "..."
            
            cmd = ['ollama', 'run', model, prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
            
            if result.returncode == 0:
                response = result.stdout.strip()
                return response
            else:
                return f"âŒ Ollama error: {result.stderr}"
                
        except subprocess.TimeoutExpired:
            return "â°ï¸ Ollama timeout - trying faster response..."
        except Exception as e:
            return f"âŒ Query error: {e}"
    
    def query_sam_model(self, prompt):
        """Query SAM model if available"""
        if not self.sam_available:
            return "ğŸ§  SAM model not available - using Ollama only"
        
        # Simulate SAM response (in real implementation, would call C SAM model)
        sam_responses = [
            f"Through SAM's neural architecture, I process '{prompt}' using pattern recognition and adaptive learning.",
            f"SAM analyzes '{prompt}' through multi-stage processing: character â†’ word â†’ phrase â†’ response.",
            f"Using SAM's self-associative memory, I recognize patterns in '{prompt}' and generate contextual responses."
        ]
        
        import hashlib
        index = int(hashlib.md5(prompt.encode()).hexdigest(), 16) % len(sam_responses)
        return sam_responses[index]
    
    def load_knowledge_context(self, prompt):
        """Load relevant knowledge from knowledge base"""
        if not self.knowledge_available:
            return ""
        
        # Simple keyword-based knowledge retrieval
        kb_path = self.base_path / "KNOWLEDGE_BASE"
        context_parts = []
        
        # Check different knowledge files
        for kb_file in kb_path.glob("*.json"):
            try:
                with open(kb_file, 'r') as f:
                    data = json.load(f)
                    
                # Search for relevant content
                if isinstance(data, dict):
                    for key, value in data.items():
                        if any(word.lower() in str(value).lower() for word in prompt.split() if len(word) > 2):
                            context_parts.append(f"{key}: {str(value)[:100]}...")
                            
            except:
                continue
        
        return "\n".join(context_parts[:3]) if context_parts else ""
    
    def generate_response(self, user_input):
        """Generate response using available systems"""
        print(f"\nğŸ¤” Processing: '{user_input}'")
        
        # Load knowledge context
        context = self.load_knowledge_context(user_input)
        if context:
            print(f"ğŸ“š Found relevant knowledge context")
        
        # Build enhanced prompt
        enhanced_prompt = user_input
        if context:
            enhanced_prompt = f"Context: {context}\n\nQuestion: {user_input}"
        
        # Try SAM first
        if self.sam_available:
            sam_response = self.query_sam_model(user_input)
            print(f"ğŸ§  SAM: {sam_response[:50]}...")
        
        # Get Ollama response
        ollama_response = self.query_ollama_direct(enhanced_prompt)
        print(f"ğŸ¤– Ollama: {ollama_response[:50]}...")
        
        # Combine responses
        if self.sam_available:
            final_response = f"ğŸ§  SAM Analysis: {sam_response}\n\nğŸ¤– Ollama Response: {ollama_response}"
        else:
            final_response = ollama_response
        
        # Store conversation
        self.conversation_history.append({
            'timestamp': time.time(),
            'user': user_input,
            'bot': final_response,
            'systems_used': ['SAM' if self.sam_available else None, 'Ollama' if self.ollama_available else None]
        })
        
        return final_response
    
    def evaluate_performance(self, user_input, bot_response):
        """Evaluate response quality using Ollama"""
        if not self.ollama_available:
            return "ğŸ“Š Ollama not available for evaluation"
        
        eval_prompt = f"""
        Evaluate this Q&A pair on a scale of 1-10:
        
        Q: {user_input}
        A: {bot_response}
        
        Rate for: relevance, accuracy, helpfulness, coherence
        """
        
        evaluation = self.query_ollama_direct(eval_prompt)
        return evaluation
    
    def save_conversation(self):
        """Save conversation history"""
        timestamp = int(time.time())
        filename = f"chatbot_conversation_{timestamp}.json"
        
        conversation_data = {
            'timestamp': timestamp,
            'session_start': self.session_start,
            'duration': time.time() - self.session_start,
            'system_status': {
                'sam_available': self.sam_available,
                'ollama_available': self.ollama_available,
                'knowledge_available': self.knowledge_available
            },
            'conversation_count': len(self.conversation_history),
            'conversations': self.conversation_history
        }
        
        with open(filename, 'w') as f:
            json.dump(conversation_data, f, indent=2)
        
        print(f"ğŸ’¾ Conversation saved to: {filename}")
        return filename
    
    def run_chatbot(self):
        """Run the interactive chatbot"""
        print(f"\nğŸš€ CHATBOT READY!")
        print(f"ğŸ’¬ Type 'quit' to exit, 'status' for system info, 'save' to save conversation")
        print(f"ğŸ¯ Available systems: {'SAM + ' if self.sam_available else ''}{'Ollama' if self.ollama_available else 'None'}")
        
        while True:
            try:
                user_input = input(f"\nğŸ‘¤ You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() == 'quit':
                    print(f"\nğŸ‘‹ Goodbye! Saving conversation...")
                    self.save_conversation()
                    break
                
                if user_input.lower() == 'status':
                    self.show_status()
                    continue
                
                if user_input.lower() == 'save':
                    self.save_conversation()
                    continue
                
                # Generate response
                start_time = time.time()
                response = self.generate_response(user_input)
                response_time = time.time() - start_time
                
                print(f"\nğŸ¤– Bot ({response_time:.2f}s):")
                print(f"{response}")
                
                # Optional evaluation
                if self.ollama_available and len(self.conversation_history) % 5 == 0:
                    print(f"\nğŸ“Š Evaluating response quality...")
                    evaluation = self.evaluate_performance(user_input, response)
                    print(f"ğŸ“ˆ Evaluation: {evaluation}")
                
            except KeyboardInterrupt:
                print(f"\n\nğŸ‘‹ Interrupted! Saving conversation...")
                self.save_conversation()
                break
            except Exception as e:
                print(f"âŒ Error: {e}")
    
    def show_status(self):
        """Show system status"""
        print(f"\nğŸ“Š SYSTEM STATUS")
        print(f"{'='*40}")
        print(f"ğŸ§  SAM Model: {'âœ… Available' if self.sam_available else 'âŒ Not Available'}")
        print(f"ğŸ¤– Ollama: {'âœ… Available' if self.ollama_available else 'âŒ Not Available'}")
        print(f"ğŸ“š Knowledge Base: {'âœ… Available' if self.knowledge_available else 'âŒ Not Available'}")
        print(f"ğŸ’¬ Conversations: {len(self.conversation_history)}")
        print(f"â±ï¸ Session Duration: {time.time() - self.session_start:.1f} seconds")
        
        if self.conversation_history:
            print(f"ğŸ“ˆ Average Response Time: {sum(c.get('response_time', 0) for c in self.conversation_history) / len(self.conversation_history):.2f}s")

def main():
    """Main function"""
    print("ğŸ¤– STREAMLINED CHATBOT INITIALIZATION")
    print("=" * 50)
    
    try:
        # Create chatbot
        chatbot = StreamlinedChatbot()
        
        # Run chatbot
        chatbot.run_chatbot()
        
    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ Chatbot interrupted by user")
    except Exception as e:
        print(f"\nâŒ Chatbot error: {e}")
    finally:
        print(f"\nğŸ‰ Streamlined chatbot session completed!")

if __name__ == "__main__":
    main()
